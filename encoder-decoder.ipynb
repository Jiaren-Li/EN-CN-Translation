{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "import tensorflow as tf\n",
    "import jieba\n",
    "import codecs\n",
    "import collections\n",
    "import sys\n",
    "from operator import itemgetter\n",
    "import tqdm\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIR_PATH=\"/home/xujingzhou/transformer-ljr/vocab.enzh\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer_encoder(sentence):\n",
    "    sentence = sentence.lower()\n",
    "    vocab=[]\n",
    "    vector=[]\n",
    "    with codecs.open(DIR_PATH,\"r\",\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            line = line[:-1]\n",
    "            vocab.append(line)\n",
    "    words=jieba.cut(sentence)\n",
    "    for word in words:\n",
    "        if not word.isspace():\n",
    "            if word in vocab:\n",
    "                temp=vocab.index(word)\n",
    "                vector.append(temp)\n",
    "    return vector\n",
    "\n",
    "def tokenizer_decoder(vector):\n",
    "    vocab=[]\n",
    "    sentence=\"\"\n",
    "    with codecs.open(DIR_PATH,\"r\",\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            line = line[:-1]\n",
    "            vocab.append(line)\n",
    "    for number in vector:\n",
    "        sentence=sentence+vocab[number]\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model cost 0.645 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'鲨鱼不会'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_encoder('鲨鱼不会咬人')\n",
    "\n",
    "tokenizer_decoder(tokenizer_encoder('鲨鱼不会咬人'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53826 ----> 伊黎\n",
      "20956 ----> 伊斯兰\n",
      "499236 ----> 国\n",
      "147367 ----> 自己\n",
      "169509 ----> 发布\n",
      "308559 ----> 视频\n",
      "633713 ----> ，\n",
      "715675 ----> 描述\n",
      "845738 ----> 人们\n",
      "364183 ----> 受到\n",
      "660631 ----> 一系列\n",
      "352438 ----> 令人发指\n",
      "574069 ----> 的\n",
      "16536 ----> 惩罚\n",
      "633713 ----> ，\n",
      "258743 ----> 包括\n",
      "701698 ----> 投掷\n",
      "482593 ----> 石块\n",
      "356174 ----> 、\n",
      "80028 ----> 从\n",
      "347281 ----> 楼上\n",
      "446614 ----> 推下去\n",
      "356174 ----> 、\n",
      "391022 ----> 斩首\n",
      "356174 ----> 、\n",
      "16327 ----> 用\n",
      "597401 ----> 十字架\n",
      "164499 ----> 钉死\n",
      "110805 ----> 。\n"
     ]
    }
   ],
   "source": [
    "sample_string = '伊黎伊斯兰国自己发布视频，描述人们受到一系列令人发指的惩罚，包括投掷石块、从楼上推下去、斩首、用十字架钉死。'\n",
    "tokenized_string=tokenizer_encoder(sample_string)\n",
    "for ts in tokenized_string:\n",
    "  print ('{} ----> {}'.format(ts, tokenizer_decoder([ts])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "伊黎\n",
      "伊斯兰\n",
      "国\n",
      "自己\n",
      "发布\n",
      "视频\n",
      "，\n",
      "描述\n",
      "人们\n",
      "受到\n",
      "一系列\n",
      "令人发指\n",
      "的\n",
      "惩罚\n",
      "，\n",
      "包括\n",
      "投掷\n",
      "石块\n",
      "、\n",
      "从\n",
      "楼上\n",
      "推下去\n",
      "、\n",
      "斩首\n",
      "、\n",
      "用\n",
      "十字架\n",
      "钉死\n",
      "。\n"
     ]
    }
   ],
   "source": [
    "words=jieba.cut(sample_string)\n",
    "for word in words:\n",
    "    print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab=[]\n",
    "with codecs.open(\"/home/xujingzhou/transformer-ljr/vocab.enzh\",\"r\",\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            line = line[:-1]\n",
    "            vocab.append(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "867964"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

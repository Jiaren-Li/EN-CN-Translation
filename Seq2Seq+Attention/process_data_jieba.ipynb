{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dfbe8554-3b44-468d-8aba-f1d0c0e07fa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import tqdm\n",
    "import numpy as np\n",
    "import codecs\n",
    "import jieba\n",
    "from collections import Counter#计数器\n",
    "import torch\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pad_sequence ,pack_padded_sequence,pad_packed_sequence\n",
    "\n",
    "from torchtext.data.utils import get_tokenizer#分词器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a76f625d-bf35-4d14-9f0e-12a07b2b37e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "UNK_IDX = 0 #未知\n",
    "PAD_IDX = 1  #\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#DEBUG = True  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "033d94a2-8f75-4af8-9f1c-31724897b2e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "987840c1-968c-424b-9d2c-5709cd76ceff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#构建词汇表\n",
    "def build_dict(sentences, max_words = 50000):\n",
    "    vocab = Counter(np.concatenate(sentences)).most_common(max_words)#最大单词数是50000\n",
    "    word_to_id = {w[0]: index + 2 for index, w in enumerate(vocab)}\n",
    "    word_to_id['UNK'] = UNK_IDX  #0\n",
    "    word_to_id['PAD'] = PAD_IDX  #1\n",
    "    id_to_word = {v: k for k, v in word_to_id.items()}\n",
    "    return word_to_id,id_to_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "246ee797-2a45-4dde-8d45-9b900780f47a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 利用词典对原始句子编码 单词->数字\n",
    "\n",
    "def encode(en_sentences, ch_sentences, en_wtoi, zh_wtoi, sort_by_len=True):\n",
    "    \n",
    "    out_en_sentences = [[en_wtoi.get(w, UNK_IDX) for w in sent] for sent in en_sentences]\n",
    "    out_ch_sentences = [[zh_wtoi.get(w, UNK_IDX) for w in sent] for sent in ch_sentences]\n",
    "        \n",
    "    \n",
    "    #返回w对应的值，否则返回UNK_IDX\n",
    "    def len_argsort(seq):#按照长度进行排序\n",
    "        return sorted(range(len(seq)), key=lambda x: len(seq[x]))\n",
    "       \n",
    "    # 把中文和英文按照同样的顺序排序\n",
    "    if sort_by_len:\n",
    "        sorted_index = len_argsort(out_en_sentences)\n",
    "        out_en_sentences = [out_en_sentences[i] for i in sorted_index]\n",
    "        out_ch_sentences = [out_ch_sentences[i] for i in sorted_index]\n",
    "        \n",
    "    return out_en_sentences, out_ch_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c854fd3c-cf01-451a-ba10-82e9561aa0b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "DIR_PATH=\"/mnt/seq2seq_att/en-zh\"\n",
    "with codecs.open(DIR_PATH+'/train.zh','r','utf-8') as f1:\n",
    "    target_text=f1.read()\n",
    "with codecs.open(DIR_PATH+'/train.en','r','utf-8') as f2:\n",
    "    source_text=f2.read()\n",
    "with codecs.open(DIR_PATH+'/test.zh','r','utf-8') as f3:\n",
    "    test_target_text=f3.read()\n",
    "with codecs.open(DIR_PATH+'/test.en','r','utf-8') as f4:\n",
    "    test_source_text=f4.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a59d617d-30a7-4e07-b4de-92227f6507b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4d483753-12c8-4d7b-8c59-eea348c211ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2897366/2897366 [01:08<00:00, 42118.28it/s]\n",
      "100%|██████████| 4001/4001 [00:00<00:00, 44646.49it/s]\n",
      "  0%|          | 0/3781317 [00:00<?, ?it/s]Building prefix dict from the default dictionary ...\n",
      "Dumping model to file cache /tmp/jieba.cache\n",
      "Loading model cost 0.960 seconds.\n",
      "Prefix dict has been built successfully.\n",
      "100%|██████████| 3781317/3781317 [13:02<00:00, 4830.89it/s] \n",
      "100%|██████████| 4001/4001 [00:00<00:00, 4426.82it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "max_length=80\n",
    "\n",
    "\n",
    "from torchtext.data.utils import get_tokenizer#分词器\n",
    "tokenizer_en = get_tokenizer('basic_english')#按空格进行分割\n",
    "train_en = []\n",
    "for sentence in tqdm.tqdm(source_text.split(\"\\n\")):\n",
    "    text= tokenizer_en(sentence.lower())\n",
    "    if len(text)>max_length:\n",
    "        text=text[0:max_length]    \n",
    "    train_en.append([\"BOS\"] + text+ [\"EOS\"])#小写\n",
    "    \n",
    "\n",
    "test_en = []\n",
    "for sentence in tqdm.tqdm(test_source_text.split(\"\\n\")):\n",
    "    text= tokenizer_en(sentence.lower())\n",
    "    if len(text)>max_length:\n",
    "        text=text[0:max_length]    \n",
    "    test_en.append([\"BOS\"] + text + [\"EOS\"])#小写\n",
    "\n",
    "\n",
    "train_zh = []\n",
    "for sentence in tqdm.tqdm(target_text.split(\"\\n\")):\n",
    "    #train_zh.append([\"BOS\"] + tokenizer_cn(sentence) + [\"EOS\"])\n",
    "    text=jieba.lcut(sentence)\n",
    "    if len(text)>max_length:\n",
    "        text=text[0:max_length]\n",
    "    train_zh.append([\"BOS\"] + text + [\"EOS\"])\n",
    "\n",
    "test_zh = []\n",
    "for sentence in tqdm.tqdm(test_target_text.split(\"\\n\")):\n",
    "    text=jieba.lcut(sentence)\n",
    "    if len(text)>max_length:\n",
    "        text=text[0:max_length]\n",
    "    test_zh.append([\"BOS\"] +text+ [\"EOS\"])\n",
    "    \n",
    "train_zh =train_zh[0:2897366] \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2c8a1bac-8bcb-4d49-b65d-99805acd877c",
   "metadata": {},
   "outputs": [],
   "source": [
    "###生成词典\n",
    "en_wtoi, en_itow = build_dict(train_en)\n",
    "zh_wtoi, zh_itow = build_dict(train_zh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7d81f183-e2c7-413e-bda5-60181a84c807",
   "metadata": {},
   "outputs": [],
   "source": [
    "###整数编码\n",
    "train_en_encode, train_zh_encode = encode(train_en, train_zh, en_wtoi, zh_wtoi)\n",
    "test_en_encode, test_zh_encode = encode(test_en, test_zh, en_wtoi, zh_wtoi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b6f886ac-0393-4b0c-9ef8-16831020d4c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_124/3022475528.py:2: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  train_en_encode_save=np.array(train_en_encode)\n",
      "/tmp/ipykernel_124/3022475528.py:5: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  train_zh_encode_save=np.array(train_zh_encode)\n",
      "/tmp/ipykernel_124/3022475528.py:8: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  test_zh_encode_save=np.array(test_zh_encode)\n",
      "/tmp/ipykernel_124/3022475528.py:11: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  test_en_encode_save=np.array(test_en_encode)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "train_en_encode_save=np.array(train_en_encode)\n",
    "np.save('train_en.npy',train_en_encode_save)\n",
    "\n",
    "train_zh_encode_save=np.array(train_zh_encode)\n",
    "np.save('train_zh.npy',train_zh_encode_save)\n",
    "\n",
    "test_zh_encode_save=np.array(test_zh_encode)\n",
    "np.save('test_zh.npy',test_zh_encode_save)\n",
    "\n",
    "test_en_encode_save=np.array(test_en_encode)\n",
    "np.save('test_en.npy',test_en_encode_save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7f16830d-171d-4fdb-a008-ae58c7f481e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('en_wtoi.npy',en_wtoi)\n",
    "np.save('en_itow.npy',en_itow)\n",
    "np.save('zh_wtoi.npy',zh_wtoi)\n",
    "np.save('zh_itow.npy',zh_itow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0d7fced-984a-423d-8a03-d50560d4ac86",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32358056-3bde-4fd0-94bc-5366eccae418",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{"cells":[{"cell_type":"markdown","metadata":{},"source":[" # 模仿前面的写一个UT"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["import tensorflow_datasets as tfds\n","import tensorflow as tf\n","import jieba\n","import codecs\n","import collections\n","import sys\n","from operator import itemgetter\n","import tqdm\n","\n","import time\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","# import os\n","# os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n","# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["DIR_PATH=\"/home/xujingzhou/en-zh\"\n"]},{"cell_type":"markdown","metadata":{},"source":[" ### 分词、清洗、建立SubwordTextEncoder\n"," > 建立过程在subwordTextEncoder_*.py中完成了\n","\n"," 模型读取与展示"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["[32346, 15384, 7, 12149, 2540, 32307]\n","How a hot day!\n","32346 ----> H\n","15384 ----> ow \n","7 ----> a \n","12149 ----> hot \n","2540 ----> day\n","32307 ----> !\n"]}],"source":["tokenizer_en = tfds.deprecated.text.SubwordTextEncoder.load_from_file(DIR_PATH+'/en')\n","tokenizer_zh = tfds.deprecated.text.SubwordTextEncoder.load_from_file(DIR_PATH+'/zh')\n","sample_string = 'How a hot day!'\n","tokenized_string=tokenizer_en.encode(sample_string)\n","print(tokenized_string)\n","original_string=tokenizer_en.decode(tokenized_string)\n","print(original_string)\n","for ts in tokenized_string:\n","  print ('{} ----> {}'.format(ts, tokenizer_en.decode([ts])))\n","# print(\"-----------\")\n","# sample_string='你好,世界!'\n","# tokenized_string=tokenizer_zh.encode(sample_string)\n","# print(tokenized_string)\n","# original_string=tokenizer_zh.decode(tokenized_string)\n","# print(original_string)\n","# for ts in tokenized_string:\n","#   print ('{} ----> {}'.format(ts, tokenizer_zh.decode([ts])))\n","#YesYesYes!!\n"]},{"cell_type":"markdown","metadata":{},"source":[" 可以看到这个模型还是有一定的问题的，比如中文标点的表示方法等等，后续可能需要通过改进分词器来提高模型效果。"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["BUFFER_SIZE = 20000\n","BATCH_SIZE = 64\n"]},{"cell_type":"markdown","metadata":{},"source":[" 将开始和结束标记（token）添加到输入和目标。"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[],"source":["def encode(lang1, lang2):\n","  lang1 = [tokenizer_en.vocab_size] + tokenizer_en.encode(\n","      lang1.numpy()) + [tokenizer_en.vocab_size+1]\n","\n","  lang2 = [tokenizer_zh.vocab_size] + tokenizer_zh.encode(\n","      lang2.numpy()) + [tokenizer_zh.vocab_size+1]\n","  \n","  return lang1, lang2\n"]},{"cell_type":"markdown","metadata":{},"source":[" 这一步似乎并不需要"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":["MAX_LENGTH = 80\n"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[],"source":["def filter_max_length(x, y, max_length=MAX_LENGTH):\n","  return tf.logical_and(tf.size(x) <= max_length,\n","                        tf.size(y) <= max_length)\n"]},{"cell_type":"markdown","metadata":{},"source":[" 创建encode函数"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[],"source":["def tf_encode(en, zh):\n","  result_en, result_zh = tf.py_function(encode, [en, zh], [tf.int64, tf.int64])\n","  result_en.set_shape([None])\n","  result_zh.set_shape([None])\n","\n","  return result_en, result_zh\n"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[],"source":["#这里需要创建train——examples和test_examples：需要用到dataset\n","with codecs.open(DIR_PATH+'/train.zh','r','utf-8') as f:\n","    zh_data=f.readlines()\n","    #讲中文标点转为英文标点\n","    zh_data=[i.replace('。', '.').replace('，', ',').replace('？', '?').replace('！', '!').replace('、',',') for i in zh_data]\n","    zh_data=tf.data.Dataset.from_tensor_slices(zh_data)\n","with codecs.open(DIR_PATH+'/train.en','r','utf-8') as f:\n","    en_data=[]\n","    # #转小写\n","    for line in f:\n","      en_data.append(line.lower())\n","    # en_data=f.readlines()\n","    en_data=tf.data.Dataset.from_tensor_slices(en_data)\n","train_data=tf.data.Dataset.zip((en_data,zh_data))\n"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["loaded\n"]}],"source":["#val_Data\n","with codecs.open(DIR_PATH+'/test.zh','r','utf-8') as f:\n","    zh_data=f.readlines()\n","    zh_data=[i.replace('。', '.').replace('，', ',').replace('？', '?').replace('！', '!').replace('、',',') for i in zh_data]\n","    zh_data=tf.data.Dataset.from_tensor_slices(zh_data)\n","with codecs.open(DIR_PATH+'/test.en','r','utf-8') as f:\n","    en_data=[]\n","    for line in f:\n","      en_data.append(line.lower())\n","    # en_data=f.readlines()\n","    en_data=tf.data.Dataset.from_tensor_slices(en_data)\n","val_data=tf.data.Dataset.zip((en_data,zh_data))\n","print(\"loaded\")"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[],"source":["train_dataset = train_data.map(tf_encode)#英中\n","train_dataset = train_dataset.filter(filter_max_length)\n","# 将数据集缓存到内存中以加快读取速度。\n","train_dataset = train_dataset.cache()\n","train_dataset = train_dataset.shuffle(BUFFER_SIZE).padded_batch(BATCH_SIZE)\n","train_dataset = train_dataset.prefetch(tf.data.experimental.AUTOTUNE)\n","\n","val_dataset = val_data.map(tf_encode)\n","val_dataset = val_dataset.filter(filter_max_length).padded_batch(BATCH_SIZE)\n"]},{"cell_type":"code","execution_count":92,"metadata":{},"outputs":[{"data":{"text/plain":["(<tf.Tensor: shape=(64, 65), dtype=int64, numpy=\n"," array([[32530,    22,  2320, ...,     0,     0,     0],\n","        [32530,  6098, 32320, ...,     0,     0,     0],\n","        [32530,     1,   346, ...,     0,     0,     0],\n","        ...,\n","        [32530,   126, 12364, ...,     0,     0,     0],\n","        [32530,     9,   595, ...,     0,     0,     0],\n","        [32530,   147,  8193, ...,     0,     0,     0]])>,\n"," <tf.Tensor: shape=(64, 76), dtype=int64, numpy=\n"," array([[31162,    42,   224, ...,     0,     0,     0],\n","        [31162,  1247, 30952, ...,     0,     0,     0],\n","        [31162,   605,    62, ...,     0,     0,     0],\n","        ...,\n","        [31162,   281,  1058, ...,     0,     0,     0],\n","        [31162,   371,   392, ...,     0,     0,     0],\n","        [31162,  5446, 12959, ...,     0,     0,     0]])>)"]},"execution_count":92,"metadata":{},"output_type":"execute_result"}],"source":["pt_batch, en_batch = next(iter(train_dataset))\n","pt_batch, en_batch\n"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[],"source":["def get_angles(pos, i, d_model):\n","  angle_rates = 1 / np.power(10000, (2 * (i//2)) / np.float32(d_model))\n","  return pos * angle_rates\n"]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[],"source":["def positional_encoding(position, d_model):\n","  angle_rads = get_angles(np.arange(position)[:, np.newaxis],\n","                          np.arange(d_model)[np.newaxis, :],\n","                          d_model)\n","  \n","  # 将 sin 应用于数组中的偶数索引（indices）；2i\n","  angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n","  \n","  # 将 cos 应用于数组中的奇数索引；2i+1\n","  angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n","    \n","  pos_encoding = angle_rads[np.newaxis, ...]\n","    \n","  return tf.cast(pos_encoding, dtype=tf.float32)\n"]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["(1, 50, 512)\n"]}],"source":["pos_encoding = positional_encoding(50, 512)\n","print (pos_encoding.shape)\n","\n","# plt.pcolormesh(pos_encoding[0], cmap='RdBu')\n","# plt.xlabel('Depth')\n","# plt.xlim((0, 512))\n","# plt.ylabel('Position')\n","# plt.colorbar()\n","# plt.show()\n"]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[],"source":["def create_padding_mask(seq):\n","  seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n","  \n","  # 添加额外的维度来将填充加到\n","  # 注意力对数（logits）。\n","  return seq[:, tf.newaxis, tf.newaxis, :]  # (batch_size, 1, 1, seq_len)\n"]},{"cell_type":"code","execution_count":17,"metadata":{},"outputs":[],"source":["# x = tf.constant([[7, 6, 0, 0, 1], [1, 2, 3, 0, 0], [0, 0, 0, 4, 5]])\n","# create_padding_mask(x)\n"]},{"cell_type":"code","execution_count":18,"metadata":{},"outputs":[],"source":["def create_look_ahead_mask(size):\n","  mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n","  return mask  # (seq_len, seq_len)\n"]},{"cell_type":"code","execution_count":19,"metadata":{},"outputs":[],"source":["# x = tf.random.uniform((1, 3))\n","# temp = create_look_ahead_mask(x.shape[1])\n","# temp\n"]},{"cell_type":"code","execution_count":20,"metadata":{},"outputs":[],"source":["def scaled_dot_product_attention(q, k, v, mask):\n","  \"\"\"计算注意力权重。\n","  q, k, v 必须具有匹配的前置维度。\n","  k, v 必须有匹配的倒数第二个维度，例如：seq_len_k = seq_len_v。\n","  虽然 mask 根据其类型（填充或前瞻）有不同的形状，\n","  但是 mask 必须能进行广播转换以便求和。\n","  \n","  参数:\n","    q: 请求的形状 == (..., seq_len_q, depth)\n","    k: 主键的形状 == (..., seq_len_k, depth)\n","    v: 数值的形状 == (..., seq_len_v, depth_v)\n","    mask: Float 张量，其形状能转换成\n","          (..., seq_len_q, seq_len_k)。默认为None。\n","    \n","  返回值:\n","    输出，注意力权重\n","  \"\"\"\n","\n","  matmul_qk = tf.matmul(q, k, transpose_b=True)  # (..., seq_len_q, seq_len_k)\n","  \n","  # 缩放 matmul_qk\n","  dk = tf.cast(tf.shape(k)[-1], tf.float32)\n","  scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n","\n","  # 将 mask 加入到缩放的张量上。\n","  if mask is not None:\n","    scaled_attention_logits += (mask * -1e9)  \n","\n","  # softmax 在最后一个轴（seq_len_k）上归一化，因此分数\n","  # 相加等于1。\n","  attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)  # (..., seq_len_q, seq_len_k)\n","\n","  output = tf.matmul(attention_weights, v)  # (..., seq_len_q, depth_v)\n","\n","  return output, attention_weights\n"]},{"cell_type":"code","execution_count":21,"metadata":{},"outputs":[],"source":["def print_out(q, k, v):\n","  temp_out, temp_attn = scaled_dot_product_attention(\n","      q, k, v, None)\n","  print ('Attention weights are:')\n","  print (temp_attn)\n","  print ('Output is:')\n","  print (temp_out)\n"]},{"cell_type":"code","execution_count":22,"metadata":{},"outputs":[],"source":["np.set_printoptions(suppress=True)\n","\n","temp_k = tf.constant([[10,0,0],\n","                      [0,10,0],\n","                      [0,0,10],\n","                      [0,0,10]], dtype=tf.float32)  # (4, 3)\n","\n","temp_v = tf.constant([[   1,0],\n","                      [  10,0],\n","                      [ 100,5],\n","                      [1000,6]], dtype=tf.float32)  # (4, 2)\n","\n","# 这条 `请求（query）符合第二个`主键（key）`，\n","# 因此返回了第二个`数值（value）`。\n","# temp_q = tf.constant([[0, 10, 0]], dtype=tf.float32)  # (1, 3)\n","# print_out(temp_q, temp_k, temp_v)\n","\n","# # %%\n","# # 这条请求符合重复出现的主键（第三第四个），\n","# # 因此，对所有的相关数值取了平均。\n","# temp_q = tf.constant([[0, 0, 10]], dtype=tf.float32)  # (1, 3)\n","# print_out(temp_q, temp_k, temp_v)\n"]},{"cell_type":"code","execution_count":23,"metadata":{},"outputs":[],"source":["# 这条请求符合第一和第二条主键，\n","# 因此，对它们的数值去了平均。\n","# temp_q = tf.constant([[10, 10, 0]], dtype=tf.float32)  # (1, 3)\n","# print_out(temp_q, temp_k, temp_v)\n","\n","# # %%\n","# temp_q = tf.constant([[0, 0, 10], [0, 10, 0], [10, 10, 0]], dtype=tf.float32)  # (3, 3)\n","# print_out(temp_q, temp_k, temp_v)\n"]},{"cell_type":"code","execution_count":24,"metadata":{},"outputs":[],"source":["class MultiHeadAttention(tf.keras.layers.Layer):\n","  def __init__(self, d_model, num_heads):\n","    super(MultiHeadAttention, self).__init__()\n","    self.num_heads = num_heads\n","    self.d_model = d_model\n","    \n","    assert d_model % self.num_heads == 0\n","    \n","    self.depth = d_model // self.num_heads\n","    \n","    self.wq = tf.keras.layers.Dense(d_model)\n","    self.wk = tf.keras.layers.Dense(d_model)\n","    self.wv = tf.keras.layers.Dense(d_model)\n","    \n","    self.dense = tf.keras.layers.Dense(d_model)\n","        \n","  def split_heads(self, x, batch_size):\n","    \"\"\"分拆最后一个维度到 (num_heads, depth).\n","    转置结果使得形状为 (batch_size, num_heads, seq_len, depth)\n","    \"\"\"\n","    x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n","    return tf.transpose(x, perm=[0, 2, 1, 3])\n","    \n","  def call(self, v, k, q, mask):\n","    batch_size = tf.shape(q)[0]\n","    \n","    q = self.wq(q)  # (batch_size, seq_len, d_model)\n","    k = self.wk(k)  # (batch_size, seq_len, d_model)\n","    v = self.wv(v)  # (batch_size, seq_len, d_model)\n","    \n","    q = self.split_heads(q, batch_size)  # (batch_size, num_heads, seq_len_q, depth)\n","    k = self.split_heads(k, batch_size)  # (batch_size, num_heads, seq_len_k, depth)\n","    v = self.split_heads(v, batch_size)  # (batch_size, num_heads, seq_len_v, depth)\n","    \n","    # scaled_attention.shape == (batch_size, num_heads, seq_len_q, depth)\n","    # attention_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k)\n","    scaled_attention, attention_weights = scaled_dot_product_attention(\n","        q, k, v, mask)\n","    \n","    scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])  # (batch_size, seq_len_q, num_heads, depth)\n","\n","    concat_attention = tf.reshape(scaled_attention, \n","                                  (batch_size, -1, self.d_model))  # (batch_size, seq_len_q, d_model)\n","\n","    output = self.dense(concat_attention)  # (batch_size, seq_len_q, d_model)\n","        \n","    return output, attention_weights\n"]},{"cell_type":"code","execution_count":25,"metadata":{},"outputs":[],"source":["# temp_mha = MultiHeadAttention(d_model=512, num_heads=8)\n","# y = tf.random.uniform((1, 60, 512))  # (batch_size, encoder_sequence, d_model)\n","# out, attn = temp_mha(y, k=y, q=y, mask=None)\n","# out.shape, attn.shape\n"]},{"cell_type":"code","execution_count":109,"metadata":{},"outputs":[],"source":["def point_wise_feed_forward_network(d_model, dff):\n","  return tf.keras.Sequential([\n","      tf.keras.layers.Dense(dff, activation='relu'),  # (batch_size, seq_len, dff)\n","      tf.keras.layers.Dense(d_model)  # (batch_size, seq_len, d_model)\n","  ])\n","\n","class ACT(tf.keras.Model):\n","    \n","    def __init__(self, batch_size, length, hidden_size):\n","        super(ACT, self).__init__()\n","        \n","        self.halting_probability = tf.zeros((batch_size, length), name='halting_probability')\n","        self.remainders = tf.zeros((batch_size, length), name=\"remainder\")\n","        self.n_updates = tf.zeros((batch_size, length), name=\"n_updates\")\n","        \n","    def call(self, pondering, halt_threshold):\n","        still_running = tf.cast(tf.less(self.halting_probability, 1.0), tf.float32)\n","\n","        # mask\n","        new_halted = tf.greater(self.halting_probability + pondering * still_running, halt_threshold)\n","        new_halted = tf.cast(new_halted, tf.float32) * still_running\n","\n","        # update mask\n","        still_running_now = tf.less_equal(self.halting_probability + pondering * still_running, halt_threshold)\n","        still_running_now = tf.cast(still_running_now, tf.float32) * still_running\n","\n","        # update halting probability\n","        self.halting_probability += pondering * still_running\n","\n","        # update remainders and halting probability\n","        self.remainders += new_halted * (1 - self.halting_probability)\n","        self.halting_probability += new_halted * self.remainders\n","\n","        # update times\n","        self.n_updates += still_running + new_halted\n","\n","        # calc update weights\n","        update_weights = pondering * still_running + new_halted * self.remainders\n","        update_weights = tf.expand_dims(update_weights, -1)\n","        \n","        return update_weights\n","    \n","    def should_continue(self, threshold) -> bool:\n","        result=tf.reduce_any(tf.less(self.halting_probability, threshold))\n","        # result = tf.cast(result, tf.bool)\n","        #展示result的数据类型\n","        return result\n","\n","def act_layer(pondering, halt_threshold,halting_probability,remainders,n_updates):\n","    still_running = tf.cast(tf.less(halting_probability, 1.0), tf.float32)\n","    # mask\n","    new_halted = tf.greater(halting_probability + pondering * still_running, halt_threshold)\n","    new_halted = tf.cast(new_halted, tf.float32) * still_running\n","\n","    # update mask\n","    still_running_now = tf.less_equal(halting_probability + pondering * still_running, halt_threshold)\n","    still_running_now = tf.cast(still_running_now, tf.float32) * still_running\n","\n","    # update halting probability\n","    halting_probability += pondering * still_running\n","\n","    # update remainders and halting probability\n","    remainders += new_halted * (1 - halting_probability)\n","    halting_probability += new_halted * remainders\n","\n","    # update times\n","    n_updates += still_running + new_halted\n","\n","    # calc update weights\n","    update_weights = pondering * still_running + new_halted * remainders\n","    update_weights = tf.expand_dims(update_weights, -1)\n","    # print(update_weights.shape)\n","    \n","    return update_weights,halting_probability,remainders,n_updates\n","\n","def should_continue(threshold,halting_probability) -> bool:\n","    result=tf.reduce_any(tf.less(halting_probability, threshold))\n","    result = tf.cast(result, tf.bool)\n","    return result\n"]},{"cell_type":"code","execution_count":117,"metadata":{},"outputs":[],"source":["class Encoder(tf.keras.layers.Layer):\n","  def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size,\n","               maximum_position_encoding, rate=0.1,max_act_steps=20,act_epsilon=0.01):#这么大？\n","    super(Encoder, self).__init__()\n","\n","    self.d_model = d_model\n","    self.num_layers = num_layers\n","    self.max_act_steps = max_act_steps\n","    self.act_epsilon=act_epsilon\n","    \n","    self.embedding = tf.keras.layers.Embedding(input_vocab_size, d_model)\n","    self.pos_encoding = positional_encoding(maximum_position_encoding, \n","                                            self.d_model)\n","    self.pondering_layer = tf.keras.layers.Dense(1, activation=tf.nn.sigmoid, use_bias=True, bias_initializer=tf.constant_initializer(1.0))\n","    \n","    self.self_attention_layers = MultiHeadAttention(d_model, num_heads)\n","    self.ffn_layers = point_wise_feed_forward_network(d_model, dff)\n","\n","    self.layer_norm1=tf.keras.layers.LayerNormalization(epsilon=1e-6)\n","    self.layer_norm2=tf.keras.layers.LayerNormalization(epsilon=1e-6)\n","\n","    self.dropout1 = tf.keras.layers.Dropout(rate)\n","    self.dropout2 = tf.keras.layers.Dropout(rate)\n","  \n","    self.output_norm=tf.keras.layers.LayerNormalization(epsilon=1e-6)\n","      \n","  def call(self, x, training, mask):\n","    # print(\"x.shape:\",x.shape)\n","    def f1(state,halting_probability,remainders,n_updates):\n","      # print(\"x\")\n","      return state,halting_probability,remainders,n_updates\n","\n","    def f2(state,previous_state,training,seq_len,step,mask,halting_probability,remainders,n_updates):\n","      # print(\"y\")\n","      state+=self.pos_encoding[:,:seq_len,:]\n","      signal=self.pos_encoding[0,:self.max_act_steps,:]\n","      signal=tf.expand_dims(signal[step,:],axis=0)\n","      # print(signal.shape)\n","      # print(state.shape)\n","      state+=signal\n","      # print(\"x\")\n","      pondering=self.pondering_layer(state)\n","      pondering=tf.squeeze(pondering,axis=-1)\n","      # print(\"x\")\n","      update_weight,halting_probability,remainders,n_updates=act_layer(pondering,1-self.act_epsilon,halting_probability,remainders,n_updates)\n","      # print(\"x\")\n","      state1=self.layer_norm1(state)\n","      state1,_=self.self_attention_layers(state1,state1,state1,mask)#这个mask有用吗？\n","      state=self.dropout1(state1+state,training=training)\n","      # print(\"x\")\n","      state2=self.layer_norm2(state)\n","      state2=self.ffn_layers(state2)\n","      state=self.dropout2(state2+state,training=training)\n","      # print(\"x\")\n","      state=state*update_weight+previous_state*(1-update_weight)\n","      return state,halting_probability,remainders,n_updates\n","\n","    seq_len = tf.shape(x)[1]\n","    batch_size= tf.shape(x)[0]\n","\n","    halting_probability = tf.zeros((batch_size, seq_len), name='halting_probability')\n","    remainders = tf.zeros((batch_size, seq_len), name=\"remainder\")\n","    n_updates = tf.zeros((batch_size, seq_len), name=\"n_updates\")\n","    \n","    # 将嵌入和位置编码相加。\n","    x = self.embedding(x)  # (batch_size, input_seq_len, d_model)\n","    x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n","    state=x\n","    previous_state=tf.zeros_like(x) \n","    # print(\"0\")\n","    for step in range(self.max_act_steps):\n","      # print(state.shape)\n","      state,halting_probability,remainders,n_updates=tf.cond(should_continue(1-self.act_epsilon,halting_probability),\n","        lambda:f2(state,previous_state,training,seq_len,step,mask,halting_probability,remainders,n_updates),\n","        lambda:f1(state,halting_probability,remainders,n_updates))\n","      # print(\"2\")\n","      previous_state=state      \n","    return self.output_norm(state),n_updates,remainders\n","\n"]},{"cell_type":"code","execution_count":28,"metadata":{},"outputs":[],"source":["class Encoder(tf.keras.layers.Layer):\n","  def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size,\n","               maximum_position_encoding, rate=0.1,max_act_steps=20,act_epsilon=0.01):#这么大行吗？\n","    super(Encoder, self).__init__()\n","\n","    self.d_model = d_model\n","    self.num_layers = num_layers\n","    self.max_act_steps = max_act_steps\n","    self.act_epsilon=act_epsilon\n","    \n","    self.embedding = tf.keras.layers.Embedding(input_vocab_size, d_model)\n","    self.pos_encoding = positional_encoding(maximum_position_encoding, \n","                                            self.d_model)\n","    self.pondering_layer = tf.keras.layers.Dense(1, activation=tf.nn.sigmoid, use_bias=True, bias_initializer=tf.constant_initializer(1.0))\n","    \n","    self.self_attention_layers = MultiHeadAttention(d_model, num_heads)\n","    self.ffn_layers = point_wise_feed_forward_network(d_model, dff)\n","\n","    self.layer_norm1=tf.keras.layers.LayerNormalization(epsilon=1e-6)\n","    self.layer_norm2=tf.keras.layers.LayerNormalization(epsilon=1e-6)\n","\n","    self.dropout1 = tf.keras.layers.Dropout(rate)\n","    self.dropout2 = tf.keras.layers.Dropout(rate)\n","  \n","    self.output_norm=tf.keras.layers.LayerNormalization(epsilon=1e-6)\n","\n","        \n","  def call(self, x, training, mask):\n","\n","    seq_len = tf.shape(x)[1]\n","    batch_size= tf.shape(x)[0]\n","    act=ACT(batch_size,seq_len,self.d_model)\n","    # 将嵌入和位置编码相加。\n","    x = self.embedding(x)  # (batch_size, input_seq_len, d_model)\n","    x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n","    state=x\n","    previous_state=tf.zeros_like(x) \n","    # print(\"0\")\n","    for step in range(self.max_act_steps):\n","      if act.should_continue(1-self.act_epsilon):\n","        break\n","      state+=self.pos_encoding[:,:seq_len,:]\n","      #时间步\n","      signal=self.pos_encoding[0,:self.max_act_steps,:]\n","      signal=tf.expand_dims(signal[step,:],axis=0)\n","      # print(signal.shape)\n","      # print(state.shape)\n","      state+=signal\n","\n","      pondering=self.pondering_layer(state)\n","      pondering=tf.squeeze(pondering,axis=-1)\n","\n","      update_weight=act(pondering,1-self.act_epsilon)\n","      #多头自注意力\n","      state1=self.layer_norm1(state)\n","      state1,_=self.self_attention_layers(state1,state1,state1,mask)#这个mask有用吗？\n","      state=self.dropout1(state1+state,training=training)\n","\n","      state2=self.layer_norm2(state)\n","      state2=self.ffn_layers(state2)\n","      state=self.dropout2(state2+state,training=training)\n","\n","      state=state*update_weight+previous_state*(1-update_weight)\n","      previous_state=state\n","      \n","    return self.output_norm(state),act.n_updates,act.remainders"]},{"cell_type":"code","execution_count":29,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["(64, 62, 512)\n"]}],"source":["sample_encoder = Encoder(num_layers=2, d_model=512, num_heads=8, \n","                         dff=2048, input_vocab_size=8500,\n","                         maximum_position_encoding=10000)\n","\n","sample_encoder_output,_,_ = sample_encoder(tf.random.uniform((64, 62)), \n","                                       training=False, mask=None)\n","\n","print (sample_encoder_output.shape)  # (batch_size, input_seq_len, d_model)\n"]},{"cell_type":"code","execution_count":135,"metadata":{},"outputs":[],"source":["class Decoder(tf.keras.layers.Layer):\n","  def __init__(self, num_layers, d_model, num_heads, dff, target_vocab_size,\n","               maximum_position_encoding, rate=0.1,max_act_steps=20,act_epsilon=0.01):\n","    super(Decoder, self).__init__()\n","\n","    self.d_model = d_model\n","    self.num_layers = num_layers\n","    self.max_act_steps = max_act_steps\n","    self.act_epsilon=act_epsilon\n","    \n","    self.embedding = tf.keras.layers.Embedding(target_vocab_size, d_model)\n","    self.pos_encoding = positional_encoding(maximum_position_encoding, d_model)\n","    self.pondering_layer = tf.keras.layers.Dense(1, activation=tf.nn.sigmoid, use_bias=True, bias_initializer=tf.constant_initializer(1.0))\n","    \n","    self.self_attention_layers = MultiHeadAttention(d_model, num_heads)\n","    self.enc_dec_attention_layers = MultiHeadAttention(d_model, num_heads)\n","    self.ffn_layers = point_wise_feed_forward_network(d_model, dff)\n","\n","    self.layer_norm1=tf.keras.layers.LayerNormalization(epsilon=1e-6)\n","    self.layer_norm2=tf.keras.layers.LayerNormalization(epsilon=1e-6)\n","    self.layer_norm3=tf.keras.layers.LayerNormalization(epsilon=1e-6)\n","\n","    self.dropout1 = tf.keras.layers.Dropout(rate)\n","    self.dropout2 = tf.keras.layers.Dropout(rate)\n","    self.dropout3 = tf.keras.layers.Dropout(rate)\n","    self.dropout=tf.keras.layers.Dropout(rate)\n","  \n","    self.output_norm=tf.keras.layers.LayerNormalization(epsilon=1e-6)\n","    \n","    \n","    \n","  def call(self, x, enc_output, training, \n","           look_ahead_mask, padding_mask):\n","    def f1(state,halting_probability,remainders,n_updates):\n","      # print(\"x\")\n","      return state,halting_probability,remainders,n_updates\n","    def f2(state,previous_state,training,seq_len,step,halting_probability,remainders,n_updates,enc_output,look_ahead_mask,padding_mask):\n","      state+=self.pos_encoding[:,:seq_len,:]\n","      # print(\"x\")\n","      signal=self.pos_encoding[0,:self.max_act_steps,:]\n","      signal=tf.expand_dims(signal[step,:],axis=0)\n","      state+=signal\n","      # print(\"x\")\n","      pondering=self.pondering_layer(state)\n","      pondering=tf.squeeze(pondering,axis=-1)\n","      # print(\"x\")\n","      update_weight,halting_probability,remainders,n_updates=act_layer(pondering,1-self.act_epsilon,halting_probability,remainders,n_updates)\n","      # print(\"x\")\n","      state1=self.layer_norm1(state)\n","      state1,attention_weights=self.self_attention_layers(state1,state1,state1,look_ahead_mask)\n","      state=self.dropout1(state1+state,training=training)\n","      # print(\"x\")\n","      state2=self.layer_norm2(state)\n","      state2,attention_weights=self.enc_dec_attention_layers(enc_output,enc_output,state2,padding_mask)\n","      state=self.dropout2(state2+state,training=training)\n","      # print(\"x\")\n","      state3=self.layer_norm3(state)\n","      state3=self.ffn_layers(state3)\n","      state=self.dropout3(state3+state,training=training)\n","      # print(\"x\")\n","      state=state*update_weight+previous_state*(1-update_weight)\n","      return state,halting_probability,remainders,n_updates\n","\n","    seq_len = tf.shape(x)[1]\n","    batch_size= tf.shape(x)[0]\n","\n","    halting_probability = tf.zeros((batch_size, seq_len), name='halting_probability')\n","    remainders = tf.zeros((batch_size, seq_len), name=\"remainder\")\n","    n_updates = tf.zeros((batch_size, seq_len), name=\"n_updates\")\n","\n","    x=self.embedding(x)\n","    x*=tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n","    x=self.dropout(x,training=training)\n","    \n","    state=x\n","    previous_state=tf.zeros_like(x)\n","    # print(state.shape)\n","    for step in range(self.max_act_steps):\n","      # print(step)\n","      state,halting_probability,remainders,n_updates=tf.cond(should_continue(1-self.act_epsilon,halting_probability),\n","        lambda:f2(state,previous_state,training,seq_len,step,halting_probability,remainders,n_updates,enc_output,look_ahead_mask,padding_mask),\n","        lambda:f1(state,halting_probability,remainders,n_updates))\n","      previous_state=state\n","\n","    state=self.output_norm(state)\n","    return state,n_updates,remainders\n"]},{"cell_type":"code","execution_count":31,"metadata":{},"outputs":[],"source":["# sample_decoder = Decoder(num_layers=2, d_model=512, num_heads=8, \n","#                          dff=2048, target_vocab_size=8000,\n","#                          maximum_position_encoding=5000)\n","\n","# output, attn = sample_decoder(tf.random.uniform((64, 26)), \n","#                               enc_output=sample_encoder_output, \n","#                               training=False, look_ahead_mask=None, \n","#                               padding_mask=None)\n","\n","# output.shape, attn['decoder_layer2_block2'].shape\n"]},{"cell_type":"code","execution_count":136,"metadata":{},"outputs":[],"source":["class Transformer(tf.keras.Model):\n","  def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size, \n","               target_vocab_size, pe_input, pe_target, rate=0.1,act_loss_weight=0.01,max_act_steps=20,act_epsilon=0.01):\n","    super(Transformer, self).__init__()\n","\n","    self.encoder = Encoder(num_layers, d_model, num_heads, dff, \n","                           input_vocab_size, pe_input, rate)\n","\n","    self.decoder = Decoder(num_layers, d_model, num_heads, dff, \n","                           target_vocab_size, pe_target, rate)\n","\n","    self.final_layer = tf.keras.layers.Dense(target_vocab_size)\n","    self.act_loss_weight=act_loss_weight\n","    \n","  def call(self, inp, tar, training, enc_padding_mask, \n","           look_ahead_mask, dec_padding_mask):\n","    # print(\"encoder\")\n","    # print(inp.shape)\n","    enc_output, enc_ponders, enc_remainders = self.encoder(inp, training, enc_padding_mask)  # (batch_size, inp_seq_len, d_model)\n","    # dec_output.shape == (batch_size, tar_seq_len, d_model)\n","    print(\"decoder\")\n","    dec_output, dec_ponders, dec_remainders= self.decoder(\n","        tar, enc_output, training, look_ahead_mask, dec_padding_mask)\n","    final_output = self.final_layer(dec_output)  # (batch_size, tar_seq_len, target_vocab_size)\n","    enc_act_loss = tf.reduce_mean(enc_ponders + enc_remainders)\n","    dec_act_loss = tf.reduce_mean(dec_ponders + dec_remainders)\n","    act_loss = enc_act_loss + dec_act_loss\n","    return final_output,act_loss\n"]},{"cell_type":"code","execution_count":137,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["decoder\n","0\n","1\n","2\n","3\n","4\n","5\n","6\n","7\n","8\n","9\n","10\n","11\n","12\n","13\n","14\n","15\n","16\n","17\n","18\n","19\n"]},{"data":{"text/plain":["TensorShape([64, 26, 8000])"]},"execution_count":137,"metadata":{},"output_type":"execute_result"}],"source":["sample_transformer = Transformer(\n","    num_layers=2, d_model=512, num_heads=8, dff=2048, \n","    input_vocab_size=8500, target_vocab_size=8000, \n","    pe_input=10000, pe_target=6000)\n","\n","temp_input = tf.random.uniform((64, 62))\n","temp_target = tf.random.uniform((64, 26))\n","with tf.GradientTape() as tape:\n","    fn_out, _ = sample_transformer(temp_input, temp_target, training=True, \n","                                enc_padding_mask=None, \n","                                look_ahead_mask=None,\n","                                dec_padding_mask=None)\n","\n","fn_out.shape  # (batch_size, tar_seq_len, target_vocab_size)\n"]},{"cell_type":"code","execution_count":34,"metadata":{},"outputs":[],"source":["num_layers = 4\n","d_model = 128\n","dff = 512\n","num_heads = 8\n","\n","input_vocab_size = tokenizer_en.vocab_size + 2\n","target_vocab_size = tokenizer_zh.vocab_size + 2\n","dropout_rate = 0.1\n"]},{"cell_type":"code","execution_count":35,"metadata":{},"outputs":[],"source":["class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n","  def __init__(self, d_model, warmup_steps=4000):\n","    super(CustomSchedule, self).__init__()\n","    \n","    self.d_model = d_model\n","    self.d_model = tf.cast(self.d_model, tf.float32)\n","\n","    self.warmup_steps = warmup_steps\n","    \n","  def __call__(self, step):\n","    arg1 = tf.math.rsqrt(step)\n","    arg2 = step * (self.warmup_steps ** -1.5)\n","    \n","    return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\n"]},{"cell_type":"code","execution_count":36,"metadata":{},"outputs":[],"source":["learning_rate = CustomSchedule(d_model)\n","\n","optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, \n","                                     epsilon=1e-9)\n"]},{"cell_type":"code","execution_count":37,"metadata":{},"outputs":[],"source":["# temp_learning_rate_schedule = CustomSchedule(d_model)\n","\n","# plt.plot(temp_learning_rate_schedule(tf.range(40000, dtype=tf.float32)))\n","# plt.ylabel(\"Learning Rate\")\n","# plt.xlabel(\"Train Step\")\n"]},{"cell_type":"code","execution_count":38,"metadata":{},"outputs":[],"source":["loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n","    from_logits=True, reduction='none')\n"]},{"cell_type":"code","execution_count":39,"metadata":{},"outputs":[],"source":["def loss_function(real, pred):\n","  mask = tf.math.logical_not(tf.math.equal(real, 0))\n","  loss_ = loss_object(real, pred)\n","\n","  mask = tf.cast(mask, dtype=loss_.dtype)\n","  loss_ *= mask\n","  \n","  return tf.reduce_mean(loss_)\n"]},{"cell_type":"code","execution_count":40,"metadata":{},"outputs":[],"source":["train_loss = tf.keras.metrics.Mean(name='train_loss')\n","train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(\n","    name='train_accuracy')\n"]},{"cell_type":"code","execution_count":41,"metadata":{},"outputs":[],"source":["def create_masks(inp, tar):\n","  # 编码器填充遮挡\n","  enc_padding_mask = create_padding_mask(inp)\n","  \n","  # 在解码器的第二个注意力模块使用。\n","  # 该填充遮挡用于遮挡编码器的输出。\n","  dec_padding_mask = create_padding_mask(inp)\n","  \n","  # 在解码器的第一个注意力模块使用。\n","  # 用于填充（pad）和遮挡（mask）解码器获取到的输入的后续标记（future tokens）。\n","  look_ahead_mask = create_look_ahead_mask(tf.shape(tar)[1])\n","  dec_target_padding_mask = create_padding_mask(tar)\n","  combined_mask = tf.maximum(dec_target_padding_mask, look_ahead_mask)\n","  \n","  return enc_padding_mask, combined_mask, dec_padding_mask\n"]},{"cell_type":"code","execution_count":138,"metadata":{},"outputs":[],"source":["transformer = Transformer(num_layers, d_model, num_heads, dff,\n","                          input_vocab_size, target_vocab_size, \n","                          pe_input=input_vocab_size, \n","                          pe_target=target_vocab_size,\n","                          rate=dropout_rate)\n"]},{"cell_type":"code","execution_count":139,"metadata":{},"outputs":[],"source":["checkpoint_path = \"/home/xujingzhou/checkpoint_u1\"\n","\n","ckpt = tf.train.Checkpoint(transformer=transformer,\n","                           optimizer=optimizer)\n","\n","ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n","\n","# 如果检查点存在，则恢复最新的检查点。\n","if ckpt_manager.latest_checkpoint:\n","  ckpt.restore(ckpt_manager.latest_checkpoint)\n","  print ('Latest checkpoint restored!!')\n"]},{"cell_type":"code","execution_count":140,"metadata":{},"outputs":[],"source":["EPOCHS = 1\n"]},{"cell_type":"code","execution_count":141,"metadata":{},"outputs":[],"source":["# 该 @tf.function 将追踪-编译 train_step 到 TF 图中，以便更快地\n","# 执行。该函数专用于参数张量的精确形状。为了避免由于可变序列长度或可变\n","# 批次大小（最后一批次较小）导致的再追踪，使用 input_signature 指定\n","# 更多的通用形状。\n","\n","train_step_signature = [\n","    tf.TensorSpec(shape=(None, None), dtype=tf.int64),\n","    tf.TensorSpec(shape=(None, None), dtype=tf.int64),\n","]\n","\n","@tf.function()\n","def train_step(inp, tar):\n","  tar_inp = tar[:, :-1]\n","  tar_real = tar[:, 1:]\n","  \n","  enc_padding_mask, combined_mask, dec_padding_mask = create_masks(inp, tar_inp)\n","  with tf.GradientTape() as tape:\n","    predictions,act_loss= transformer(inp, tar_inp, \n","                                 True, \n","                                 enc_padding_mask, \n","                                 combined_mask, \n","                                 dec_padding_mask)\n","    loss = loss_function(tar_real, predictions)+act_loss\n","  print(\"loss\")\n","  gradients = tape.gradient(loss, transformer.trainable_variables)    \n","  optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n","  train_loss(loss)\n","  train_accuracy(tar_real, predictions)\n"]},{"cell_type":"code","execution_count":142,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["decoder\n","0\n","1\n","2\n","3\n","4\n","5\n","6\n","7\n","8\n","9\n","10\n","11\n","12\n","13\n","14\n","15\n","16\n","17\n","18\n","19\n","decoder\n","0\n","1\n","2\n","3\n","4\n","5\n","6\n","7\n","8\n","9\n","10\n","11\n","12\n","13\n","14\n","15\n","16\n","17\n","18\n","19\n","Epoch 1 Batch 0 Loss 10.4931 Accuracy 0.0000\n","Epoch 1 Loss 10.4931 Accuracy 0.0000\n","Time taken for 1 epoch: 116.052738904953 secs\n","\n"]}],"source":["for epoch in range(EPOCHS):\n","  start = time.time()\n","  \n","  train_loss.reset_states()\n","  train_accuracy.reset_states()\n","  \n","  # inp -> portuguese, tar -> english\n","  for (batch, (inp, tar)) in enumerate(val_dataset):\n","    train_step(inp, tar)\n","    \n","    if batch % 50 == 0:\n","      print ('Epoch {} Batch {} Loss {:.4f} Accuracy {:.4f}'.format(\n","          epoch + 1, batch, train_loss.result(), train_accuracy.result()))\n","    break\n","      \n","  if (epoch + 1) % 2 == 0:\n","    ckpt_save_path = ckpt_manager.save()\n","    print ('Saving checkpoint for epoch {} at {}'.format(epoch+1,\n","                                                         ckpt_save_path))\n","    \n","  print ('Epoch {} Loss {:.4f} Accuracy {:.4f}'.format(epoch + 1, \n","                                                train_loss.result(), \n","                                                train_accuracy.result()))\n","\n","  print ('Time taken for 1 epoch: {} secs\\n'.format(time.time() - start))\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#en和zh弄反了呜呜呜呜呜呜呜呜呜呜呜！！\n","def evaluate(inp_sentence):\n","  start_token = [tokenizer_en.vocab_size]\n","  end_token = [tokenizer_en.vocab_size + 1]\n","  \n","  # 输入语句是葡萄牙语，增加开始和结束标记\n","  inp_sentence = start_token + tokenizer_en.encode(inp_sentence) + end_token\n","  encoder_input = tf.expand_dims(inp_sentence, 0)\n","  \n","  # 因为目标是英语，输入 transformer 的第一个词应该是\n","  # 英语的开始标记。\n","  decoder_input = [tokenizer_zh.vocab_size]\n","  output = tf.expand_dims(decoder_input, 0)\n","    \n","  for i in range(MAX_LENGTH):\n","    enc_padding_mask, combined_mask, dec_padding_mask = create_masks(\n","        encoder_input, output)\n","  \n","    # predictions.shape == (batch_size, seq_len, vocab_size)\n","    predictions,_= transformer(encoder_input, \n","                                                 output,\n","                                                 False,\n","                                                 enc_padding_mask,\n","                                                 combined_mask,\n","                                                 dec_padding_mask)\n","    \n","    # 从 seq_len 维度选择最后一个词\n","    predictions = predictions[: ,-1:, :]  # (batch_size, 1, vocab_size)\n","\n","    predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)\n","    \n","    # 如果 predicted_id 等于结束标记，就返回结果\n","    if predicted_id == tokenizer_zh.vocab_size+1:\n","      return tf.squeeze(output, axis=0)\n","    \n","    # 连接 predicted_id 与输出，作为解码器的输入传递到解码器。\n","    output = tf.concat([output, predicted_id], axis=-1)\n","\n","  return tf.squeeze(output, axis=0)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#这个是错误的错误！是妥协！\n","# def evaluate(inp_sentence):\n","#   start_token = [tokenizer_zh.vocab_size]\n","#   end_token = [tokenizer_zh.vocab_size + 1]\n","  \n","#   # 输入语句是葡萄牙语，增加开始和结束标记\n","#   inp_sentence = start_token + tokenizer_en.encode(inp_sentence) + end_token\n","#   encoder_input = tf.expand_dims(inp_sentence, 0)\n","  \n","#   # 因为目标是英语，输入 transformer 的第一个词应该是\n","#   # 英语的开始标记。\n","#   decoder_input = [tokenizer_en.vocab_size]\n","#   output = tf.expand_dims(decoder_input, 0)\n","    \n","#   for i in range(MAX_LENGTH):\n","#     enc_padding_mask, combined_mask, dec_padding_mask = create_masks(\n","#         encoder_input, output)\n","  \n","#     # predictions.shape == (batch_size, seq_len, vocab_size)\n","#     predictions, attention_weights = transformer(encoder_input, \n","#                                                  output,\n","#                                                  False,\n","#                                                  enc_padding_mask,\n","#                                                  combined_mask,\n","#                                                  dec_padding_mask)\n","    \n","#     # 从 seq_len 维度选择最后一个词\n","#     predictions = predictions[: ,-1:, :]  # (batch_size, 1, vocab_size)\n","\n","#     predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)\n","    \n","#     # 如果 predicted_id 等于结束标记，就返回结果\n","#     if predicted_id == tokenizer_en.vocab_size+1:\n","#       return tf.squeeze(output, axis=0), attention_weights\n","    \n","#     # 连接 predicted_id 与输出，作为解码器的输入传递到解码器。\n","#     output = tf.concat([output, predicted_id], axis=-1)\n","\n","#   return tf.squeeze(output, axis=0), attention_weights\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def translate(sentence):\n","  result= evaluate(sentence)\n","  print(result)\n","  predicted_sentence = tokenizer_zh.decode([i for i in result \n","                                            if i < tokenizer_zh.vocab_size])  \n","\n","  print('Input: {}'.format(sentence))\n","  print('Predicted translation: {}'.format(predicted_sentence))\n","\n"]}],"metadata":{"interpreter":{"hash":"31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"},"kernelspec":{"display_name":"Python 3.6.9 64-bit","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.9"},"orig_nbformat":4},"nbformat":4,"nbformat_minor":2}
